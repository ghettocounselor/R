## Please download and install the latest version from http://h2o.ai/download/
h2o::h2o.no_progress()
h2o_train = h2o::as.h2o(train)
h2o_test = h2o::as.h2o(test)
gbm <- h2o::h2o.gbm(y = "y",
x = setdiff(colnames(train), "y"),
training_frame = h2o_train,
nfolds = 5)
h2o_train = h2o::as.h2o(train)
h2o::h2o.init()
x_train <- as.matrix(train[,-1]); y=train[,1]; y_train <- keras::to_categorical(as.numeric(y)-1);
`%>%` <- magrittr::`%>%`
nn <- keras::keras_model_sequential() %>%
keras::layer_dense(units = 16,kernel_initializer = "uniform",activation = 'relu',
input_shape = NCOL(x_train))%>%
keras::layer_dense(units = 16,kernel_initializer = "uniform", activation='relu') %>%
keras::layer_dense(units = length(levels(train[,1])),activation='softmax')
nn %>% keras::compile(optimizer='rmsprop',loss='categorical_crossentropy',metrics=c('accuracy'))
nn %>% keras::fit(x_train,y_train,epochs = 20,batch_size = 1028)
scores_and_ntiles <- prepare_scores_and_ntiles(datasets=list("train","test"),
dataset_labels = list("train data","test data"),
models = list("rf","mnl"),
#, "gbm","nn"),
model_labels = list("random forest","multinomial logit"),
#                   "gradient boosted trees","artificial neural network"),
target_column="y",
ntiles=100)
test_size = 0.3
train_index =  sample(seq(1, nrow(bank)),size = (1 - test_size)*nrow(bank) ,replace = F)
train = bank[train_index,]
test = bank[-train_index,]
library('mlr')
trainTask <- mlr::makeClassifTask(data = train, target = "y")
testTask <- mlr::makeClassifTask(data = test, target = "y")
# mlr::configureMlr() # this line is needed when using mlr without loading it (mlr::)
task = mlr::makeClassifTask(data = train, target = "y")
#install.packages('randomForest')
library('randomForest')
lrn = mlr::makeLearner("classif.randomForest", predict.type = "prob")
rf = mlr::train(lrn, task)
library('lattice')
library('ggplot2')
library('caret')
fitControl <- caret::trainControl(method = "cv",number = 2,classProbs=TRUE)
# mnl model using glmnet package
#install.packages('glmnet')
library('Matrix')
library('foreach')
library('glmnet')
#install.packages('e1071')
library('e1071')
mnl = caret::train(y ~.,data = train, method = "glmnet",trControl = fitControl)
x_train <- as.matrix(train[,-1]); y=train[,1]; y_train <- keras::to_categorical(as.numeric(y)-1);
`%>%` <- magrittr::`%>%`
nn <- keras::keras_model_sequential() %>%
keras::layer_dense(units = 16,kernel_initializer = "uniform",activation = 'relu',
input_shape = NCOL(x_train))%>%
keras::layer_dense(units = 16,kernel_initializer = "uniform", activation='relu') %>%
keras::layer_dense(units = length(levels(train[,1])),activation='softmax')
## train models using keras.
install.packages('keras')
## train models using keras.
#install.packages('keras')
library('keras')
x_train <- as.matrix(train[,-1]); y=train[,1]; y_train <- keras::to_categorical(as.numeric(y)-1);
`%>%` <- magrittr::`%>%`
nn <- keras::keras_model_sequential() %>%
keras::layer_dense(units = 16,kernel_initializer = "uniform",activation = 'relu',
input_shape = NCOL(x_train))%>%
keras::layer_dense(units = 16,kernel_initializer = "uniform", activation='relu') %>%
keras::layer_dense(units = length(levels(train[,1])),activation='softmax')
library(modelplotr)
scores_and_ntiles <- prepare_scores_and_ntiles(datasets=list("train","test"),
dataset_labels = list("train data","test data"),
models = list("rf","mnl"),
#, "gbm","nn"),
model_labels = list("random forest","multinomial logit"),
#                   "gradient boosted trees","artificial neural network"),
target_column="y",
ntiles=100)
scores_and_ntiles
plot_input <- plotting_scope(prepared_input = scores_and_ntiles,
select_model_label = "gradient boosted trees",
select_dataset_label = "test data")
plot_input <- plotting_scope(prepared_input = scores_and_ntiles,
select_model_label = "random forest",
#select_model_label = "gradient boosted trees",
select_dataset_label = "test data")
plot_input
plot_cumgains(data = plot_input)
# plot the cumulative gains plot and annotate the plot at percentile = 20
plot_cumgains(data = plot_input,highlight_ntile = 20)
plot_cumlift(data = plot_input,highlight_ntile = 20)
# plot the response plot and annotate the plot at ntile = 10
plot_response(data = plot_input,highlight_ntile = 10)
# plot the cumulative response plot and annotate the plot at decile = 3
plot_cumresponse(data = plot_input,highlight_ntile = 30)
plot_multiplot(data = plot_input,highlight_ntile=2,
save_fig = TRUE,save_fig_filename = 'Selection model Term Deposits')
#Profit plot - automatically highlighting the ntile where profit is maximized!
plot_profit(data=plot_input,fixed_costs=75000,variable_costs_per_unit=50,profit_per_unit=250)
# set plotting scope to model comparison
plot_input <- plotting_scope(prepared_input = scores_and_ntiles,scope = "compare_models")
# plot the cumulative response plot and annotate the plot at ntile 20
plot_cumresponse(data = plot_input,highlight_ntile = 20)
setwd("~/R_programing/R_randomCode/modelplotr - predictive models")
h2o::h2o.init()
h2o::h2o.init()
h2o::h2o.init()
h2o::h2o.init()
h2o.init(ip = "localhost", port = 54321, name = NA_character_,
startH2O = TRUE, forceDL = FALSE, enable_assertions = TRUE,
license = NULL, nthreads = -1, max_mem_size = NULL,
min_mem_size = NULL, ice_root = tempdir(), log_dir = NA_character_,
log_level = NA_character_, strict_version_check = TRUE,
proxy = NA_character_, https = FALSE, insecure = FALSE,
username = NA_character_, password = NA_character_,
cookies = NA_character_, context_path = NA_character_,
ignore_config = FALSE, extra_classpath = NULL,
jvm_custom_args = NULL, bind_to_localhost = TRUE)
if ("package:h2o" %in% search()) { detach("package:h2o", unload=TRUE) }
if ("h2o" %in% rownames(installed.packages())) { remove.packages("h2o") }
pkgs <- c("RCurl","jsonlite")
for (pkg in pkgs) {
if (! (pkg %in% rownames(installed.packages()))) { install.packages(pkg) }
}
install.packages("h2o", type="source", repos=(c("http://h2o-release.s3.amazonaws.com/h2o/latest_stable_R")))
install.packages("h2o", type = "source", repos = (c("http://h2o-release.s3.amazonaws.com/h2o/latest_stable_R")))
library(h2o)
localH2O = h2o.init()
h2o.init(ip = "localhost", port = 54321, name = NA_character_,
startH2O = TRUE, forceDL = FALSE, enable_assertions = TRUE,
license = NULL, nthreads = -1, max_mem_size = NULL,
min_mem_size = NULL, ice_root = tempdir(), log_dir = NA_character_,
log_level = NA_character_, strict_version_check = TRUE,
proxy = NA_character_, https = FALSE, insecure = FALSE,
username = NA_character_, password = NA_character_,
cookies = NA_character_, context_path = NA_character_,
ignore_config = FALSE, extra_classpath = NULL,
jvm_custom_args = NULL, bind_to_localhost = TRUE)
install.packages("RDocumentation")
library('RDocumentation')
dyn.load('/Library/Java/JavaVirtualMachines/jdk1.8.0_162.jdk/Contents/Home/jre/lib/server/libjvm.dylib')
library('RDocumentation')
#install.packages('modelplotr')
library('modelplotr')
zipname = 'https://modelplot.github.io/img/bank-additional.zip'
csvname = 'bank-additional/bank-additional-full.csv'
temp <- tempfile()
download.file(zipname,temp, mode="wb")
bank <- read.table(unzip(temp,csvname),sep=";", stringsAsFactors=FALSE,header = T)
unlink(temp)
bank <- bank[,c('y','duration','campaign','pdays','previous','euribor3m')]
bank$y[bank$y=='yes'] <- 'term.deposit'
bank$y <- as.factor(bank$y)
str(bank)
test_size = 0.3
train_index =  sample(seq(1, nrow(bank)),size = (1 - test_size)*nrow(bank) ,replace = F)
train = bank[train_index,]
test = bank[-train_index,]
library('mlr')
trainTask <- mlr::makeClassifTask(data = train, target = "y")
testTask <- mlr::makeClassifTask(data = test, target = "y")
# mlr::configureMlr() # this line is needed when using mlr without loading it (mlr::)
task = mlr::makeClassifTask(data = train, target = "y")
#train models using mlr...
install.packages('ParamHelpers')
install.packages("ParamHelpers")
install.packages("ParamHelpers")
library('ParamHelpers')
#install.packages('mlr')
library('mlr')
trainTask <- mlr::makeClassifTask(data = train, target = "y")
testTask <- mlr::makeClassifTask(data = test, target = "y")
# mlr::configureMlr() # this line is needed when using mlr without loading it (mlr::)
task = mlr::makeClassifTask(data = train, target = "y")
#install.packages('randomForest')
library('randomForest')
lrn = mlr::makeLearner("classif.randomForest", predict.type = "prob")
rf = mlr::train(lrn, task)
library('lattice')
library('ggplot2')
library('caret')
fitControl <- caret::trainControl(method = "cv",number = 2,classProbs=TRUE)
library('Matrix')
library('foreach')
library('glmnet')
#install.packages('e1071')
library('e1071')
mnl = caret::train(y ~.,data = train, method = "glmnet",trControl = fitControl)
## Warning in load(system.file("models", "models.RData", package = "caret")):
## strings not representable in native encoding will be translated to UTF-8
head('mnl')
## Warning in load(system.file("models", "models.RData", package = "caret")):
## strings not representable in native encoding will be translated to UTF-8
summary('mnl')
## Warning in load(system.file("models", "models.RData", package = "caret")):
## strings not representable in native encoding will be translated to UTF-8
mnl
# train models using h2o...
#install.packages('h2o')
library('h2o')
## train models using keras.
#install.packages('keras')
library('keras')
x_train <- as.matrix(train[,-1]); y=train[,1]; y_train <- keras::to_categorical(as.numeric(y)-1);
`%>%` <- magrittr::`%>%`
nn <- keras::keras_model_sequential() %>%
keras::layer_dense(units = 16,kernel_initializer = "uniform",activation = 'relu',
input_shape = NCOL(x_train))%>%
keras::layer_dense(units = 16,kernel_initializer = "uniform", activation='relu') %>%
keras::layer_dense(units = length(levels(train[,1])),activation='softmax')
nn %>% keras::compile(optimizer='rmsprop',loss='categorical_crossentropy',metrics=c('accuracy'))
nn %>% keras::fit(x_train,y_train,epochs = 20,batch_size = 1028)
x_train
library(modelplotr)
scores_and_ntiles <- prepare_scores_and_ntiles(datasets=list("train","test"),
dataset_labels = list("train data","test data"),
models = list("rf","mnl","nn"),
#, "gbm"),
model_labels = list("random forest","multinomial logit","artificial neural network"),
#                   "gradient boosted trees"),
target_column="y",
ntiles=100)
library("magrittr", lib.loc="/Library/Frameworks/R.framework/Versions/3.5/Resources/library")
library(magrittr)
scores_and_ntiles <- prepare_scores_and_ntiles(datasets=list("train","test"),
dataset_labels = list("train data","test data"),
models = list("rf","mnl","nn"),
#, "gbm"),
model_labels = list("random forest","multinomial logit","artificial neural network"),
#                   "gradient boosted trees"),
target_column="y",
ntiles=100)
x_train <- as.matrix(train[,-1]); y=train[,1]; y_train <- keras::to_categorical(as.numeric(y)-1);
`%>%` <- magrittr::`%>%`
nn <- keras::keras_model_sequential() %>%
keras::layer_dense(units = 16,kernel_initializer = "uniform",activation = 'relu',
input_shape = NCOL(x_train))%>%
keras::layer_dense(units = 16,kernel_initializer = "uniform", activation='relu') %>%
keras::layer_dense(units = length(levels(train[,1])),activation='softmax')
nn %>% keras::compile(optimizer='rmsprop',loss='categorical_crossentropy',metrics=c('accuracy'))
nn %>% keras::fit(x_train,y_train,epochs = 20,batch_size = 1028)
x_train
scores_and_ntiles <- prepare_scores_and_ntiles(datasets=list("train","test"),
dataset_labels = list("train data","test data"),
models = list("rf","mnl","nn"),
#, "gbm"),
model_labels = list("random forest","multinomial logit","artificial neural network"),
#                   "gradient boosted trees"),
target_column="y",
ntiles=100)
nn
x_train <- as.matrix(train[,-1]); y=train[,1]; y_train <- keras::to_categorical(as.numeric(y)-1);
`%do%` <- magrittr::`do%`
x_train <- as.matrix(train[,-1]); y=train[,1]; y_train <- keras::to_categorical(as.numeric(y)-1);
`%do%` <- magrittr::`%do%`
library(dplyr)
x_train <- as.matrix(train[,-1]); y=train[,1]; y_train <- keras::to_categorical(as.numeric(y)-1);
`%do%` <- magrittr::`%do%`
nn <- keras::keras_model_sequential()
x_train <- as.matrix(train[,-1]); y=train[,1]; y_train <- keras::to_categorical(as.numeric(y)-1);
`%>%` <- magrittr::`%>%`
nn <- keras::keras_model_sequential() %>%
keras::layer_dense(units = 16,kernel_initializer = "uniform",activation = 'relu',
input_shape = NCOL(x_train))%>%
keras::layer_dense(units = 16,kernel_initializer = "uniform", activation='relu') %>%
keras::layer_dense(units = length(levels(train[,1])),activation='softmax')
#install.packages("dplyr")    # alternative installation of the %>%
#library(dplyr)    # alternative, this also loads %>%
# install tensorflow
devtools::install_github("rstudio/tensorflow")
#install.packages("dplyr")    # alternative installation of the %>%
#library(dplyr)    # alternative, this also loads %>%
# install tensorflow
install_github("rstudio/tensorflow")
#install.packages("dplyr")    # alternative installation of the %>%
#library(dplyr)    # alternative, this also loads %>%
# install tensorflow
install_tensorflow()
library("tensorflow", lib.loc="/Library/Frameworks/R.framework/Versions/3.5/Resources/library")
#install.packages("dplyr")    # alternative installation of the %>%
#library(dplyr)    # alternative, this also loads %>%
# install tensorflow
#install_tensorflow()
library('tensorflow')
x_train <- as.matrix(train[,-1]); y=train[,1]; y_train <- keras::to_categorical(as.numeric(y)-1);
`%>%` <- magrittr::`%>%`
nn <- keras::keras_model_sequential() %>%
keras::layer_dense(units = 16,kernel_initializer = "uniform",activation = 'relu',
input_shape = NCOL(x_train))%>%
keras::layer_dense(units = 16,kernel_initializer = "uniform", activation='relu') %>%
keras::layer_dense(units = length(levels(train[,1])),activation='softmax')
nn %>% keras::compile(optimizer='rmsprop',loss='categorical_crossentropy',metrics=c('accuracy'))
nn %>% keras::fit(x_train,y_train,epochs = 20,batch_size = 1028)
x_train
nn
nn
scores_and_ntiles <- prepare_scores_and_ntiles(datasets=list("train","test"),
dataset_labels = list("train data","test data"),
models = list("rf","mnl","nn"),
#, "gbm"),
model_labels = list("random forest","multinomial logit","artificial neural network"),
#                   "gradient boosted trees"),
target_column="y",
ntiles=100)
plot_input <- plotting_scope(prepared_input = scores_and_ntiles,
#select_model_label = "random forest",
select_model_label = "gradient boosted trees",
select_dataset_label = "test data")
plot_input <- plotting_scope(prepared_input = scores_and_ntiles,
select_model_label = "random forest",
#select_model_label = "gradient boosted trees",
select_dataset_label = "test data")
plot_input
# plot the cumulative gains plot
plot_cumgains(data = plot_input)
# plot the cumulative lift plot and annotate the plot at percentile = 20
plot_cumlift(data = plot_input,highlight_ntile = 20)
# plot the cumulative gains plot and annotate the plot at percentile = 20
plot_cumgains(data = plot_input,highlight_ntile = 20)
plot_multiplot(data = plot_input,highlight_ntile=2,
save_fig = TRUE,save_fig_filename = 'Selection model Term Deposits')
getwd()
plot_multiplot(data = plot_input,highlight_ntile=2,
save_fig = TRUE,save_fig_filename = '/Users/markloessi/R_programing/R_randomCode/modelplotr - predictive models/Selection model Term Deposits')
#
#Profit plot - automatically highlighting the ntile where profit is maximized!
plot_profit(data=plot_input,fixed_costs=75000,variable_costs_per_unit=50,profit_per_unit=250)
plot_profit(data=plot_input,fixed_costs=75000,
variable_costs_per_unit=50,profit_per_unit=250,
save_fig_filename = '/Users/markloessi/R_programing/R_randomCode/modelplotr - predictive models/Selection model Term Deposits - Profit'))
##
plot_profit(data=plot_input,fixed_costs=75000,
variable_costs_per_unit=50,profit_per_unit=250,
save_fig_filename = '/Users/markloessi/R_programing/R_randomCode/modelplotr - predictive models/Selection model Term Deposits - Profit'))
plot_profit(data=plot_input,fixed_costs=75000,
variable_costs_per_unit=50,profit_per_unit=250,
save_fig_filename = '/Users/markloessi/R_programing/R_randomCode/modelplotr - predictive models/Selection model Term Deposits - Profit')
plot_input <- plotting_scope(prepared_input = scores_and_ntiles,
scope = "compare_models"
)
# plot the cumulative response plot and annotate the plot at ntile 20
plot_cumresponse(data = plot_input,highlight_ntile = 20)
plot_cumresponse(data = plot_input,highlight_ntile = 20,
save_fig_filename = '/Users/markloessi/R_programing/R_randomCode/modelplotr - predictive models/Selection model Term Deposits - Cumul response')
bank
install_keras()
install_keras()
x_train <- as.matrix(train[,-1]); y=train[,1]; y_train <- keras::to_categorical(as.numeric(y)-1);
`%>%` <- magrittr::`%>%`
nn <- keras::keras_model_sequential() %>%
keras::layer_dense(units = 16,kernel_initializer = "uniform",activation = 'relu',
input_shape = NCOL(x_train))%>%
keras::layer_dense(units = 16,kernel_initializer = "uniform", activation='relu') %>%
keras::layer_dense(units = length(levels(train[,1])),activation='softmax')
nn %>% keras::compile(optimizer='rmsprop',loss='categorical_crossentropy',metrics=c('accuracy'))
nn %>% keras::fit(x_train,y_train,epochs = 20,batch_size = 1028)
x_train
nn
scores_and_ntiles <- prepare_scores_and_ntiles(datasets=list("train","test"),
dataset_labels = list("train data","test data"),
models = list("rf","mnl","nn"),
#, "gbm"),
model_labels = list("random forest","multinomial logit","artificial neural network"),
#                   "gradient boosted trees"),
target_column="y",
ntiles=100)
plot_input <- plotting_scope(prepared_input = scores_and_ntiles,
#select_model_label = "random forest",
select_model_label = "gradient boosted trees",
select_dataset_label = "test data")
# look at dataframe
plot_input
# plot the cumulative gains plot
plot_cumgains(data = plot_input)
plot_cumgains(data = plot_input,
save_fig_filename = '/Users/markloessi/R_programing/R_randomCode/modelplotr - predictive models/Selection model Term Deposits - Cumul gains')
plot_cumgains(data = plot_input,highlight_ntile = 20,
save_fig_filename = '/Users/markloessi/R_programing/R_randomCode/modelplotr - predictive models/Selection model Term Deposits - Cumul gains c20')
plot_cumlift(data = plot_input,highlight_ntile = 20,
save_fig_filename = '/Users/markloessi/R_programing/R_randomCode/modelplotr - predictive models/Selection model Term Deposits - Cumul lift c20')
# plot the response plot and annotate the plot at ntile = 10
plot_response(data = plot_input,highlight_ntile = 10,
save_fig_filename = '/Users/markloessi/R_programing/R_randomCode/modelplotr - predictive models/Selection model Term Deposits - Response c10')
# plot the cumulative response plot and annotate the plot at decile = 3
plot_cumresponse(data = plot_input,highlight_ntile = 30)
# transform datasets and model objects into scored data and calculate ntiles
# preparation steps
# adjusted for my scoring method above MSL
# can we see everything?
rf
mnl
nn
gbm
View(testTask)
nn      # generated with keras
scores_and_ntiles <- prepare_scores_and_ntiles(datasets=list("train","test"),
dataset_labels = list("train data","test data"),
models = list("rf","mnl","nn"),
# for h2o "gbm"),
model_labels = list("random forest","multinomial logit","artificial neural network"),
# for h2o "gradient boosted trees"),
target_column="y",
ntiles=100)
plot_input <- plotting_scope(prepared_input = scores_and_ntiles,
select_model_label = "random forest",
#select_model_label = "gradient boosted trees",
select_dataset_label = "test data")
plot_input <- plotting_scope(prepared_input = scores_and_ntiles,
#select_model_label = "random forest",
select_model_label = "gradient boosted trees",
select_dataset_label = "test data")
plot_input <- plotting_scope(prepared_input = scores_and_ntiles,
select_model_label = "random forest",
#need h2o running to use this   #select_model_label = "gradient boosted trees",
select_dataset_label = "test data")
plot_cumresponse(data = plot_input,highlight_ntile = 30,
save_fig_filename = '/Users/markloessi/R_programing/R_randomCode/modelplotr - predictive models/Selection model Term Deposits - Cumul Response c30')
plot_multiplot(data = plot_input,highlight_ntile=2,
save_fig = TRUE,
save_fig_filename = '/Users/markloessi/R_programing/R_randomCode/modelplotr - predictive models/Selection model Term Deposits')
plot_profit(data=plot_input,fixed_costs=75000,
variable_costs_per_unit=50,profit_per_unit=250,
save_fig_filename = '/Users/markloessi/R_programing/R_randomCode/modelplotr - predictive models/Selection model Term Deposits - Profit')
plot_input <- plotting_scope(prepared_input = scores_and_ntiles,
scope = "compare_models",
save_fig_filename = '/Users/markloessi/R_programing/R_randomCode/modelplotr - predictive models/Selection model Term Deposits - Profit cScope')
plot_input <- plotting_scope(prepared_input = scores_and_ntiles,
scope = "compare_models",
save_fig_filename = '/Users/markloessi/R_programing/R_randomCode/modelplotr - predictive models/Selection model Term Deposits - Profit cScope')
plot_input <- plotting_scope(prepared_input = scores_and_ntiles,
scope = "compare_models")
plot_input <- plotting_scope(prepared_input = scores_and_ntiles,
scope = "compare_models")
# set plotting scope to model comparison
# Get more out of modelplotr: using different scopes, highlight ntiles, customize text & colors.
plot_input <- plotting_scope(prepared_input = scores_and_ntiles,scope = "compare_models")
plot_cumresponse(data = plot_input,highlight_ntile = 20,
save_fig_filename = '/Users/markloessi/R_programing/R_randomCode/modelplotr - predictive models/Selection model Term Deposits - Cumul Response Profit cScope')
install.packages("h2o", repos=(c("http://h2o-release.s3.amazonaws.com/h2o/rel-jacobi/2/R", getOption("repos"))))
install.packages("h2o", repos = (c("http://h2o-release.s3.amazonaws.com/h2o/rel-jacobi/2/R", getOption("repos"))))
library('h2o')
localh2o = h2o.init(silentUpgrade=TRUE)
localH2O = h2o.init(silentUpgrade=TRUE)
h2o.init()
localH2O = h2o.init(silentUpgrade=TRUE)
h2o.init(ip = "localhost", port = 54321, name = NA_character_,
startH2O = TRUE, forceDL = FALSE, enable_assertions = TRUE,
license = NULL, nthreads = -1, max_mem_size = NULL,
min_mem_size = NULL, ice_root = tempdir(), log_dir = NA_character_,
log_level = NA_character_, strict_version_check = TRUE,
proxy = NA_character_, https = FALSE, insecure = FALSE,
username = NA_character_, password = NA_character_,
cookies = NA_character_, context_path = NA_character_,
ignore_config = FALSE, extra_classpath = NULL,
jvm_custom_args = NULL, bind_to_localhost = TRUE)
demo(h2o.kmeans)
demo(h2o.kmeans)
localH2O = h2o.init(ip = "localhost", port = 54321, name = NA_character_,
startH2O = TRUE, forceDL = FALSE, enable_assertions = TRUE,
license = NULL, nthreads = -1, max_mem_size = NULL,
min_mem_size = NULL, ice_root = tempdir(), log_dir = NA_character_,
log_level = NA_character_, strict_version_check = TRUE,
proxy = NA_character_, https = FALSE, insecure = FALSE,
username = NA_character_, password = NA_character_,
cookies = NA_character_, context_path = NA_character_,
ignore_config = FALSE, extra_classpath = NULL,
jvm_custom_args = NULL, bind_to_localhost = TRUE)
localH2O = h2o.init(ip = "localhost", port = 54321, name = NA_character_,
startH2O = TRUE, forceDL = FALSE, enable_assertions = TRUE,
license = NULL, nthreads = -1, max_mem_size = NULL,
min_mem_size = NULL, ice_root = tempdir(), log_dir = NA_character_,
log_level = NA_character_, strict_version_check = TRUE,
proxy = NA_character_, https = FALSE, insecure = FALSE,
username = NA_character_, password = NA_character_,
cookies = NA_character_, context_path = NA_character_,
ignore_config = FALSE, extra_classpath = NULL,
jvm_custom_args = NULL, bind_to_localhost = TRUE)
localH2O = h2o.init(ip = "localhost", port = 54321, name = NA_character_,
startH2O = TRUE, forceDL = FALSE, enable_assertions = TRUE,
license = NULL, nthreads = -1, max_mem_size = NULL,
min_mem_size = NULL, ice_root = tempdir(), log_dir = NA_character_,
log_level = NA_character_, strict_version_check = TRUE,
proxy = NA_character_, https = FALSE, insecure = FALSE,
username = NA_character_, password = NA_character_,
cookies = NA_character_, context_path = NA_character_,
ignore_config = FALSE, extra_classpath = NULL,
jvm_custom_args = NULL, bind_to_localhost = TRUE)
library("h2o", lib.loc="/Library/Frameworks/R.framework/Versions/3.5/Resources/library")
localH2O = h2o.init(ip = "localhost", port = 54321, name = NA_character_,
startH2O = TRUE, forceDL = FALSE, enable_assertions = TRUE,
license = NULL, nthreads = -1, max_mem_size = NULL,
min_mem_size = NULL, ice_root = tempdir(), log_dir = NA_character_,
log_level = NA_character_, strict_version_check = TRUE,
proxy = NA_character_, https = FALSE, insecure = FALSE,
username = NA_character_, password = NA_character_,
cookies = NA_character_, context_path = NA_character_,
ignore_config = FALSE, extra_classpath = NULL,
jvm_custom_args = NULL, bind_to_localhost = TRUE)
localH2O = h2o.init(ip = "localhost", port = 54321, name = NA_character_,
startH2O = TRUE, forceDL = FALSE, enable_assertions = TRUE,
license = NULL, nthreads = -1, max_mem_size = NULL,
min_mem_size = NULL, ice_root = tempdir(), log_dir = NA_character_,
log_level = NA_character_, strict_version_check = TRUE,
proxy = NA_character_, https = FALSE, insecure = FALSE,
username = NA_character_, password = NA_character_,
cookies = NA_character_, context_path = NA_character_,
ignore_config = FALSE, extra_classpath = NULL,
jvm_custom_args = NULL, bind_to_localhost = TRUE)
library("h2o", lib.loc="/Library/Frameworks/R.framework/Versions/3.5/Resources/library")
demo(h2o.kmeans)
